{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fraud_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSu8YLbNm76k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0859c51e-f3ef-4eee-d14c-7fa36e4b0318"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# https://www.kaggle.com/dalpozz/test-different-sampling-techniques/data\n",
        "data = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# show the contents\n",
        "print(data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Time        V1        V2  ...       V28  Amount  Class\n",
            "0            0.0 -1.359807 -0.072781  ... -0.021053  149.62    0.0\n",
            "1            0.0  1.191857  0.266151  ...  0.014724    2.69    0.0\n",
            "2            1.0 -1.358354 -1.340163  ... -0.059752  378.66    0.0\n",
            "3            1.0 -0.966272 -0.185226  ...  0.061458  123.50    0.0\n",
            "4            2.0 -1.158233  0.877737  ...  0.215153   69.99    0.0\n",
            "5            2.0 -0.425966  0.960523  ...  0.081080    3.67    0.0\n",
            "6            4.0  1.229658  0.141004  ...  0.005168    4.99    0.0\n",
            "7            7.0 -0.644269  1.417964  ... -1.085339   40.80    0.0\n",
            "8            7.0 -0.894286  0.286157  ...  0.142404   93.20    0.0\n",
            "9            9.0 -0.338262  1.119593  ...  0.083076    3.68    0.0\n",
            "10          10.0  1.449044 -1.176339  ...  0.016253    7.80    0.0\n",
            "11          10.0  0.384978  0.616109  ... -0.054337    9.99    0.0\n",
            "12          10.0  1.249999 -1.221637  ...  0.042422  121.50    0.0\n",
            "13          11.0  1.069374  0.287722  ...  0.021293   27.50    0.0\n",
            "14          12.0 -2.791855 -0.327771  ... -0.030154   58.80    0.0\n",
            "15          12.0 -0.752417  0.345485  ...  0.129394   15.99    0.0\n",
            "16          12.0  1.103215 -0.040296  ...  0.037051   12.99    0.0\n",
            "17          13.0 -0.436905  0.918966  ...  0.131024    0.89    0.0\n",
            "18          14.0 -5.401258 -5.450148  ...  0.949594   46.80    0.0\n",
            "19          15.0  1.492936 -1.029346  ...  0.007602    5.00    0.0\n",
            "20          16.0  0.694885 -1.361819  ...  0.063499  231.71    0.0\n",
            "21          17.0  0.962496  0.328461  ... -0.014605   34.09    0.0\n",
            "22          18.0  1.166616  0.502120  ... -0.011418    2.28    0.0\n",
            "23          18.0  0.247491  0.277666  ...  0.250475   22.75    0.0\n",
            "24          22.0 -1.946525 -0.044901  ...  0.014600    0.89    0.0\n",
            "25          22.0 -2.074295 -0.121482  ...  0.243232   26.43    0.0\n",
            "26          23.0  1.173285  0.353498  ...  0.030041   41.88    0.0\n",
            "27          23.0  1.322707 -0.174041  ...  0.028822   16.00    0.0\n",
            "28          23.0 -0.414289  0.905437  ...  0.152665   33.00    0.0\n",
            "29          23.0  1.059387 -0.175319  ...  0.024220   12.99    0.0\n",
            "...          ...       ...       ...  ...       ...     ...    ...\n",
            "198119  132340.0 -1.737777 -1.005122  ... -0.033112  100.00    0.0\n",
            "198120  132340.0  1.903380  0.487348  ... -0.054817    7.56    0.0\n",
            "198121  132340.0 -2.267418  2.645760  ... -0.172177    3.85    0.0\n",
            "198122  132340.0 -2.238187  0.503508  ... -0.278079   69.42    0.0\n",
            "198123  132341.0 -0.217757 -0.583285  ...  0.043732  265.80    0.0\n",
            "198124  132341.0 -2.502849 -3.139830  ... -0.260651    9.75    0.0\n",
            "198125  132342.0  1.780672 -0.533056  ... -0.026391  167.38    0.0\n",
            "198126  132342.0  1.817376 -2.218846  ... -0.019178  243.00    0.0\n",
            "198127  132343.0  2.028665 -0.428935  ... -0.058373    9.99    0.0\n",
            "198128  132343.0 -1.116146  0.228841  ...  0.272520   79.00    0.0\n",
            "198129  132343.0 -1.312948 -0.533500  ... -0.037174    0.89    0.0\n",
            "198130  132344.0 -1.281057 -0.061983  ...  0.083942   74.00    0.0\n",
            "198131  132344.0 -1.569400  1.776951  ...  0.382243   29.00    0.0\n",
            "198132  132344.0  1.999643 -0.017433  ... -0.045789    6.99    0.0\n",
            "198133  132344.0  0.266755  0.983669  ...  0.319472   19.99    0.0\n",
            "198134  132345.0 -0.983647  0.826357  ...  0.036791  224.64    0.0\n",
            "198135  132346.0  2.070799  0.128552  ... -0.076497    1.99    0.0\n",
            "198136  132346.0  2.215098 -0.934804  ... -0.081707   20.00    0.0\n",
            "198137  132347.0 -1.971680  0.113222  ... -0.065088   57.85    0.0\n",
            "198138  132348.0 -0.381516 -0.015593  ... -0.325701   39.95    0.0\n",
            "198139  132348.0  2.032707 -0.065290  ... -0.072217    2.58    0.0\n",
            "198140  132348.0  2.013836 -0.201319  ... -0.036418    6.99    0.0\n",
            "198141  132349.0  1.872611  0.435537  ... -0.006672    1.00    0.0\n",
            "198142  132349.0  2.004888  0.139182  ... -0.042010   18.54    0.0\n",
            "198143  132349.0 -0.156717 -0.002403  ...  0.117414    5.00    0.0\n",
            "198144  132350.0 -0.256123 -0.439863  ... -0.000066   89.00    0.0\n",
            "198145  132350.0 -0.541163 -0.527384  ... -0.068479   11.84    0.0\n",
            "198146  132351.0 -0.899455 -1.355703  ...  0.194485  229.90    0.0\n",
            "198147  132351.0 -2.228278  1.265204  ... -0.174821    2.31    0.0\n",
            "198148  132351.0  2.081874  0.034182  ...       NaN     NaN    NaN\n",
            "\n",
            "[198149 rows x 31 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGhEnUm892VQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5VNhsI2_lKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only use the 'Amount' and 'V1', ..., 'V28' features\n",
        "features = ['Amount'] + ['V%d' % number for number in range(1, 29)]\n",
        "\n",
        "# The target variable which we would like to predict, is the 'Class' variable\n",
        "target = 'Class'\n",
        "\n",
        "# Now create an X variable (containing the features) and an y variable (containing only the target variable)\n",
        "X = data[features]\n",
        "y = data[target]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwBjjuGRBWO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(X):\n",
        "  \"\"\"\n",
        "    Make the distribution of the values of each variable similar by subtracting \n",
        "    the mean and by dividing by the standard deviation.\n",
        "  \"\"\"\n",
        "  \n",
        "  for feature in X.columns:\n",
        "    X[feature] -= X[feature].mean()\n",
        "    X[feature] /= X[feature].std()\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXmW7rIns53_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Define the splitter for splitting the data in a train set and a test set\n",
        "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
        "\n",
        "# Loop through the splits (only one)\n",
        "for train_indices, test_indices in splitter.split(X, y):\n",
        "    # Select the train and test data\n",
        "    X_train, y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
        "    X_test, y_test = X.iloc[test_indices], y.iloc[test_indices]\n",
        "    \n",
        "    # Normalize the data\n",
        "    X_train = normalize(X_train)\n",
        "    X_test = normalize(X_test)\n",
        "    \n",
        "    # Fit and predict!\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # And finally: show the results\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}